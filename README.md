# ğŸ§  PyTorch Generative Models Lab

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Master in MBD (Data Science)** â€” Faculty of Sciences and Technology of Tangier (FSTT)  
> **Author:** Chaimae Bouassab  
> **Date:** December 2025

---

## ğŸ“‹ Overview

This laboratory explores three fundamental **generative model architectures** in deep learning using PyTorch:

| Model | Type | Key Characteristic |
|-------|------|-------------------|
| **Autoencoder (AE)** | Deterministic | Reconstruction-focused |
| **Variational Autoencoder (VAE)** | Probabilistic | Generation-focused |
| **Generative Adversarial Network (GAN)** | Adversarial | High-quality image synthesis |

---

## ğŸ¯ Learning Objectives

Through this lab, I learned to:

- âœ… Build and train encoder-decoder architectures
- âœ… Understand the difference between deterministic and probabilistic latent spaces
- âœ… Implement the reparameterization trick for VAEs
- âœ… Design Generator and Discriminator networks for GANs
- âœ… Visualize and interpret latent space representations
- âœ… Compare reconstruction quality vs. generative capability

---

## ğŸ“ Project Structure

```
PyTorch-Generative-Models-Lab/
â”œâ”€â”€ atelier4.py                          # Main experiment script
â”œâ”€â”€ README.md                            # This file
â”œâ”€â”€ mnist image.png                      # Dataset samples
â”œâ”€â”€ auto encoder build.png               # AE architecture
â”œâ”€â”€ latent space.png                     # Latent space visualization
â”œâ”€â”€ VISUALIZING LATENT SPACES.png        # AE vs VAE latent comparison
â”œâ”€â”€ training vae.png                     # VAE training curves
â”œâ”€â”€ quality of reconstruction.png        # Reconstruction comparison
â”œâ”€â”€ EVALUATION COMPARING AE vs VAE.png   # Model comparison
â”œâ”€â”€ GAN 1.png â†’ GAN 5.png               # GAN training progression
â””â”€â”€ completed tasks.png                  # Project checklist
```

---

# ğŸ”¬ Part 1: Autoencoders (AE & VAE)

## ğŸ“Š Dataset: MNIST Handwritten Digits

![MNIST Dataset Samples](mnist%20image.png)

**What I learned:** MNIST is a classic benchmark dataset containing 60,000 training images of handwritten digits (0-9). Each image is 28Ã—28 pixels in grayscale. This dataset is ideal for learning generative models because it's simple enough to train quickly but complex enough to demonstrate meaningful generation.

---

## ğŸ—ï¸ Autoencoder Architecture

![Autoencoder Build](auto%20encoder%20build.png)

### ğŸ“š Key Concepts Learned

**Architecture Overview:**
- **Encoder:** Compresses input image (784D) â†’ latent representation (32D)
- **Decoder:** Reconstructs image from latent representation (32D â†’ 784D)
- **Bottleneck:** Forces the network to learn efficient representations

**Training Results:**
```
Epoch  5/30 - Loss: 0.163295
Epoch 10/30 - Loss: 0.152845
Epoch 15/30 - Loss: 0.149103
Epoch 20/30 - Loss: 0.146307
Epoch 25/30 - Loss: 0.143597
Epoch 30/30 - Loss: 0.143597 âœ“
```

**ğŸ’¡ Insight:** The autoencoder learns to compress images by ~96% (784 â†’ 32 dimensions) while preserving enough information to reconstruct them. The decreasing loss shows successful learning.

---

## ğŸ“ˆ VAE Training Dynamics

![VAE Training Curves](training%20vae.png)

### ğŸ“š Key Concepts Learned

**VAE Loss Components:**
1. **Reconstruction Loss:** How well the model reconstructs inputs
2. **KL Divergence:** Regularizes latent space to follow N(0,1)

**Training Results:**
```
Epoch  5/30 - Total Loss: 137.72 - KL: 6.12
Epoch 10/30 - Total Loss: 131.27 - KL: 6.39
Epoch 15/30 - Total Loss: 128.16 - KL: 6.58
Epoch 20/30 - Total Loss: 126.10 - KL: 6.74
Epoch 25/30 - Total Loss: 124.48 - KL: 6.82
Epoch 30/30 - Total Loss: 124.67 - KL: 6.89 âœ“
```

**ğŸ’¡ Insight:** The KL divergence increases slightly during training, indicating the model is learning a more structured latent space. This is the trade-off VAE makes: slightly worse reconstruction for better generation capability.

---

## ğŸ—ºï¸ Latent Space Visualization

![Latent Space](latent%20space.png)

![Latent Space Comparison](VISUALIZING%20LATENT%20SPACES.png)

### ğŸ“š Key Concepts Learned

| Aspect | Autoencoder | VAE |
|--------|-------------|-----|
| **Clustering** | Tight, discrete clusters | Overlapping, continuous |
| **Gaps** | Empty regions between clusters | Filled, smooth space |
| **Interpolation** | May produce invalid outputs | Smooth transitions |
| **Sampling** | Cannot sample randomly | Can sample from N(0,1) |

**ğŸ’¡ Insight:** The VAE's latent space is **continuous and structured** thanks to KL divergence regularization. This means:
- Any point in latent space maps to a valid digit
- Interpolating between two points produces smooth morphing
- Random sampling generates realistic new digits

---

## ğŸ” Reconstruction Quality Comparison

![Reconstruction Quality](quality%20of%20reconstruction.png)

### ğŸ“š Key Concepts Learned

**Observations:**
- **AE Reconstructions:** Sharper, more detailed, better pixel-wise accuracy
- **VAE Reconstructions:** Slightly blurrier but maintains digit structure

**Why the difference?**
- AE optimizes **only** for reconstruction â†’ achieves lower MSE
- VAE optimizes for reconstruction **AND** latent regularity â†’ trades some sharpness for structured latent space

**ğŸ’¡ Insight:** This is the fundamental **reconstruction vs. generation trade-off**. If you need faithful reconstruction, use AE. If you need to generate new samples, use VAE.

---

## âš–ï¸ AE vs VAE Evaluation

![AE vs VAE Comparison](EVALUATION%20COMPARING%20AE%20vs%20VAE.png)

### ğŸ“š Summary of Key Differences

| Criterion | Autoencoder (AE) | Variational AE (VAE) |
|-----------|------------------|----------------------|
| **Encoding** | Deterministic | Probabilistic (Î¼, Ïƒ) |
| **Latent Space** | Unstructured | Regularized to N(0,1) |
| **Loss Function** | MSE only | MSE + KL Divergence |
| **Reconstruction** | â­â­â­â­â­ Excellent | â­â­â­â­ Good |
| **Generation** | âŒ Cannot generate | âœ… Can generate new samples |
| **Interpolation** | May fail | Smooth and meaningful |

### ğŸ§  Core Learning

> **"AE learns to compress; VAE learns to generate."**

The key innovation of VAE is the **reparameterization trick**: instead of outputting a single latent vector, the encoder outputs Î¼ and Ïƒ, then samples z = Î¼ + Ïƒ Ã— Îµ (where Îµ ~ N(0,1)). This allows gradients to flow through the sampling operation.

---

# ğŸ® Part 2: Generative Adversarial Networks (DCGAN)

## ğŸ“Š Dataset: Abstract Art Gallery

**Dataset Details:**
- **Source:** Kaggle (Abstract Art Gallery by Bryan)
- **Images:** 2,872 abstract art paintings
- **Resolution:** 64Ã—64 RGB
- **Challenge:** More complex than MNIST (color, texture, composition)

---

## ğŸ—ï¸ DCGAN Architecture

### Generator (G)
```
Input: Random noise z ~ N(0,1), shape (100, 1, 1)
       â†“
ConvTranspose2d: (100) â†’ (512, 4, 4)   + BatchNorm + ReLU
       â†“
ConvTranspose2d: (512) â†’ (256, 8, 8)   + BatchNorm + ReLU
       â†“
ConvTranspose2d: (256) â†’ (128, 16, 16) + BatchNorm + ReLU
       â†“
ConvTranspose2d: (128) â†’ (64, 32, 32)  + BatchNorm + ReLU
       â†“
ConvTranspose2d: (64)  â†’ (3, 64, 64)   + Tanh
       â†“
Output: Fake image, shape (3, 64, 64), range [-1, 1]
```

### Discriminator (D)
```
Input: Image (real or fake), shape (3, 64, 64)
       â†“
Conv2d: (3)   â†’ (64, 32, 32)  + LeakyReLU(0.2)
       â†“
Conv2d: (64)  â†’ (128, 16, 16) + BatchNorm + LeakyReLU(0.2)
       â†“
Conv2d: (128) â†’ (256, 8, 8)   + BatchNorm + LeakyReLU(0.2)
       â†“
Conv2d: (256) â†’ (512, 4, 4)   + BatchNorm + LeakyReLU(0.2)
       â†“
Conv2d: (512) â†’ (1, 1, 1)     + Sigmoid
       â†“
Output: Probability [0, 1] (0 = fake, 1 = real)
```

**Model Parameters:**
- Generator: **3,576,704** parameters
- Discriminator: **2,765,568** parameters

---

## ğŸ“ˆ GAN Training Progression

The following images show how the Generator improves over training epochs:

### Epoch 1-20: Early Training
![GAN Early Stage](GAN%201.png)

**What I observed:** Early outputs are noisy and lack structure. The Generator is learning basic color distributions but hasn't captured texture or composition patterns yet.

---

### Epoch 20-40: Learning Structure
![GAN Learning](GAN%202.png)

**What I observed:** Emerging shapes and color blobs. The Generator begins to understand spatial relationships but outputs are still blurry and lack detail.

---

### Epoch 40-60: Refinement
![GAN Refinement](GAN%203.png)

**What I observed:** More defined shapes and color transitions. The Generator is learning texture patterns characteristic of abstract art.

---

### Epoch 60-80: Quality Improvement
![GAN Quality](GAN4.png)

**What I observed:** Improved color harmony and composition. Generated images start to resemble plausible abstract artworks.

---

### Epoch 80-100: Final Results
![GAN Final](GAN%205.png)

**What I observed:** The Generator produces diverse, colorful abstract art with various styles. Some outputs show clear artistic patterns while others may exhibit mode collapse tendencies.

---

## ğŸ“Š GAN Training Metrics

### Training Dynamics
```
[  1/100] Loss_D: 0.2073  Loss_G: 6.2444  D(x): 0.9555  D(G(z)): 0.1280/0.0026
[...training progress...]
```

### ğŸ“š Key Metrics Explained

| Metric | Meaning | Healthy Range |
|--------|---------|---------------|
| **Loss_D** | Discriminator's classification error | 0.3 - 0.8 |
| **Loss_G** | Generator's deception failure | Varies |
| **D(x)** | D's confidence on real images | ~0.5 - 0.9 |
| **D(G(z))** | D's confidence on fake images | ~0.1 - 0.5 |

**ğŸ’¡ Insight:** GAN training is a **minimax game**:
- Generator wants to maximize D(G(z))
- Discriminator wants to maximize D(x) and minimize D(G(z))
- Balance is key: if D is too strong, G can't learn; if G is too strong, mode collapse occurs

---

## ğŸ§  Key GAN Concepts Learned

### 1. Adversarial Training
```
min_G max_D V(D,G) = E[log D(x)] + E[log(1 - D(G(z)))]
```
The Generator and Discriminator compete, pushing each other to improve.

### 2. DCGAN Best Practices
- âœ… Use strided convolutions (no pooling)
- âœ… BatchNorm in both G and D (except first/last layers)
- âœ… LeakyReLU in D, ReLU in G
- âœ… Tanh activation for G output
- âœ… Adam optimizer with Î²â‚ = 0.5

### 3. Common Issues
| Problem | Symptom | Solution |
|---------|---------|----------|
| **Mode Collapse** | G produces limited variety | Reduce learning rate, add noise |
| **Discriminator Domination** | G loss explodes | Train G more often |
| **Training Instability** | Oscillating losses | Use spectral normalization |

---

# ğŸ“ Final Summary & Conclusions

## What I Learned

### ğŸ¯ Autoencoders
1. **Compression:** AE learns efficient low-dimensional representations
2. **Limitation:** Cannot generate new samples (no regularized latent space)
3. **Use case:** Dimensionality reduction, denoising, anomaly detection

### ğŸ¯ Variational Autoencoders
1. **Probabilistic:** Encodes to distribution (Î¼, Ïƒ), not single point
2. **Reparameterization:** z = Î¼ + Ïƒ Ã— Îµ enables backpropagation
3. **Trade-off:** Reconstruction quality vs. generation capability
4. **Use case:** Image generation, latent space exploration, data augmentation

### ğŸ¯ GANs
1. **Adversarial game:** G and D improve through competition
2. **No explicit density:** GANs don't model p(x), just generate samples
3. **Sharp outputs:** Can produce higher quality images than VAE
4. **Challenge:** Training instability, mode collapse, hyperparameter sensitivity

---

## ğŸ”„ Model Comparison

| Aspect | AE | VAE | GAN |
|--------|----|----|-----|
| **Training** | Stable | Stable | Unstable |
| **Output Quality** | Blurry | Blurry | Sharp |
| **Diversity** | N/A | Good | Risk of collapse |
| **Latent Space** | Unstructured | Structured | Not explicit |
| **Loss Function** | MSE | ELBO | Adversarial |

---

## ğŸš€ How to Reproduce

### Prerequisites
```bash
pip install torch torchvision matplotlib numpy tqdm
```

### Run Experiments
```bash
# Part 1: AE and VAE on MNIST
python atelier4.py

# Part 2: GAN on Abstract Art (requires Kaggle setup)
# See notebook for Kaggle API configuration
```

---

## ğŸ“š References

1. Kingma, D. P., & Welling, M. (2014). *Auto-Encoding Variational Bayes*. ICLR.
2. Goodfellow, I., et al. (2014). *Generative Adversarial Networks*. NeurIPS.
3. Radford, A., Metz, L., & Chintala, S. (2016). *Unsupervised Representation Learning with DCGANs*. ICLR.
4. PyTorch Documentation: [pytorch.org/docs](https://pytorch.org/docs)

---

## âœ… Completed Tasks

![Completed Tasks](completed%20tasks.png)

---

<div align="center">

**ğŸ“ Lab completed successfully!**

*Master in MBD (Data Science) â€” FSTT*

</div>
